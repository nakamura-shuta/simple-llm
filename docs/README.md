# Simple LLM ドキュメント目次

教育用GPT-2スタイルTransformerモデルのRust実装に関するドキュメント集です。

**初学者向け説明**: このプロジェクトは、ChatGPTのような言語モデルの基本的な仕組みを学ぶための教材です。実際のChatGPTよりもずっとシンプルですが、同じ原理で動作します。

## 📚 ドキュメント一覧

### 🎯 プロジェクト概要

- [**CLAUDE.md**](./CLAUDE.md) - プロジェクトガイドライン（日本語）
  - プロジェクトの目的と背景
  - 実装方針
  - 今後のClaude向けの指示

### 🏗️ アーキテクチャとデザイン

- [**PROCESSING_FLOW.md**](./PROCESSING_FLOW.md) - 処理フロー説明
  - プログラムの全体的な流れ
  - 各コンポーネントの詳細動作
  - データの流れとTransformerの仕組み

### 📋 実装関連

- [**IMPLEMENTATION_CHECKLIST.md**](./IMPLEMENTATION_CHECKLIST.md) - 実装チェックリスト
  - 最低限必要な機能の確認
  - 動作検証の結果


### 🔄 発展的な内容

- [**FROM_LM_TO_CHATBOT.md**](./FROM_LM_TO_CHATBOT.md) - 言語モデルからChatGPTへ
  - 単純な次単語予測から対話システムへの進化
  - Q&A形式のデータでの学習方法

- [**REAL_LLM_COMPARISON.md**](./REAL_LLM_COMPARISON.md) - 実際のLLMとの比較
  - 本物のChatGPTとの違い
  - より高度な実装への道筋

## 🚀 クイックスタート

1. **プログラミング初心者の方**: まず[PROCESSING_FLOW.md](./PROCESSING_FLOW.md)で全体像を掴みましょう
2. **仕組みを理解したい方**: [PROCESSING_FLOW.md](./PROCESSING_FLOW.md)でシステムの構造を学びましょう
3. **実際に使ってみたい方**: ルートディレクトリのREADME.mdで使い方を確認しましょう

## 📁 プロジェクト構成

```
simple-llm/
├── src/                          # ソースコード（プログラム本体）
│   ├── lib.rs                    # ライブラリのエントリポイント（開始地点）
│   ├── main.rs                   # 実行可能ファイル（実際に動かすプログラム）
│   └── working_transformer.rs    # 動作するTransformer実装（メインの言語モデル）
├── docs/                         # ドキュメント（説明書）
│   └── （各種説明ファイル）
├── data.txt                      # 訓練データ（モデルが学習するテキスト）
├── Cargo.toml                    # Rustプロジェクト設定
├── Cargo.lock                    # 依存関係のロックファイル
├── README.md                     # プロジェクトREADME
└── CLAUDE.md                     # Claude Code向けガイドライン
```

## 🔍 主要な概念（初学者向け解説付き）

### Transformerアーキテクチャとは？
ChatGPTの心臓部分です。以下の部品から構成されています：

- **Multi-Head Attention機構**
  - 「注意機構」：文章の中で、どの単語に注目すべきかを決める仕組み
  - 「Multi-Head」：複数の視点から同時に注目することで、より豊かな理解を実現

- **Layer Normalization (Pre-LN構成)**
  - 「正規化」：データの値を一定の範囲に収める処理
  - 学習を安定させるための重要な技術

- **Position-wise Feed-Forward Networks**
  - 「Feed-Forward」：入力から出力へ一方向に情報が流れるネットワーク
  - 各位置の情報を個別に処理

- **位置エンコーディング**
  - 単語の順序情報を表現する仕組み
  - 「私は学生です」と「学生は私です」の違いを理解するために必要

### 実装の特徴
- **教育目的に最適化**: 理解しやすさを最優先
- **実際に動作する学習機能**: 本当に学習して賢くなります
- **日本語対応**: 日本語のテキストで学習・生成が可能
- **温度サンプリング**: 生成する文章の「創造性」を調整できる機能

## 🎓 学習の進め方

1. **基礎理解**: まず言語モデルが「次の単語を予測する」だけだということを理解
2. **構造理解**: Transformerがどのように単語を処理するかを学ぶ
3. **実践**: 実際にコードを動かして、学習と生成を体験
4. **応用**: Q&A形式のデータで、より対話的なシステムへ

## 📝 用語集

- **LLM (Large Language Model)**: 大規模言語モデル。ChatGPTのような言語AI
- **Transformer**: 2017年に発表された、現在の言語AIの基礎となる技術
- **トークン**: 単語やその一部。モデルが処理する最小単位
- **エポック**: 全データを1回学習すること
- **損失 (Loss)**: モデルの予測がどれだけ間違っているかを表す数値

## 🤝 貢献方法

このプロジェクトは教育目的で作成されています。改善提案やバグ報告は歓迎します。

## 📜 ライセンス

教育目的での使用を想定しています。詳細はプロジェクトのライセンスファイルを参照してください。