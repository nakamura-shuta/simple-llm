# å®Ÿéš›ã®LLMã¨ã®æ¯”è¼ƒ

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€æ•™è‚²ç”¨å®Ÿè£…ã¨å®Ÿéš›ã®LLMï¼ˆGPTã€Claudeã€LLaMAç­‰ï¼‰ã®é•ã„ã‚’èª¬æ˜ã—ã¾ã™ã€‚

**åˆå­¦è€…å‘ã‘èª¬æ˜**:
- **LLM**ã¨ã¯ã€ŒLarge Language Modelï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã€ã®ç•¥ã§ã™
- ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ç§ãŸã¡ã®ã€Œå­¦ç¿’ç”¨ã®å°ã•ãªãƒ¢ãƒ‡ãƒ«ã€ã¨ã€Œæœ¬ç‰©ã®AIï¼ˆChatGPTãªã©ï¼‰ã€ã®é•ã„ã‚’èª¬æ˜ã—ã¾ã™
- è‡ªè»¢è»Šï¼ˆå­¦ç¿’ç”¨ï¼‰ã¨F1ã‚«ãƒ¼ï¼ˆæœ¬ç‰©ï¼‰ã®é•ã„ã®ã‚ˆã†ãªã‚‚ã®ã§ã™

## ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ä»•çµ„ã¿

**ã‚ã‹ã‚Šã‚„ã™ãè¨€ã†ã¨**: AIãŒã©ã†ã‚„ã£ã¦æ–‡ç« ã‚’ä½œã‚‹ã‹ã®ä»•çµ„ã¿ãŒã€å­¦ç¿’ç”¨ã¨æœ¬ç‰©ã§ã¯å¤§ããé•ã„ã¾ã™ã€‚

### 1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®é•ã„

#### æ•™è‚²ç”¨å®Ÿè£…ï¼ˆã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰
```rust
// å˜ç´”ãªç©ºç™½åŒºåˆ‡ã‚Š
"ã“ã‚“ã«ã¡ã¯ ä¸–ç•Œ" â†’ ["ã“ã‚“ã«ã¡ã¯", "ä¸–ç•Œ"]
```

#### å®Ÿéš›ã®LLM
```python
# GPT-2/GPT-3: Byte Pair Encoding (BPE)
"ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ" â†’ ["ã“ã‚“", "ã«ã¡", "ã¯", "ä¸–ç•Œ"]

# GPT-4/Claude: Byte-level BPE ã¾ãŸã¯ SentencePiece
"Hello world" â†’ ["Hello", " world"]  # ç©ºç™½ã‚‚å«ã‚€
```

**å®Ÿéš›ã®LLMã®ç‰¹å¾´:**
- ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆBPEã€WordPieceã€SentencePieceï¼‰
  - **ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã¨ã¯**: å˜èªã‚’ã•ã‚‰ã«å°ã•ãåˆ†å‰²ã—ãŸã‚‚ã®ã€‚ã€Œé£Ÿã¹ã‚‹ã€â†’ã€Œé£Ÿã€ã€Œã¹ã‚‹ã€
- èªå½™ã‚µã‚¤ã‚º: 30,000ï½100,000ãƒˆãƒ¼ã‚¯ãƒ³
  - **èªå½™ã‚µã‚¤ã‚ºã¨ã¯**: AIãŒã€ŒçŸ¥ã£ã¦ã„ã‚‹å˜èªï¼ˆãƒ‘ãƒ¼ãƒ„ï¼‰ã€ã®æ•°
- æœªçŸ¥èªã®å‡¦ç†: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã«åˆ†è§£ã—ã¦å¯¾å¿œ
- å¤šè¨€èªå¯¾å¿œ: Unicodeå…¨ä½“ã‚’ã‚«ãƒãƒ¼

### 2. ç”Ÿæˆæˆ¦ç•¥ã®é•ã„

**åˆå­¦è€…å‘ã‘èª¬æ˜**: ã€Œæ¬¡ã®å˜èªã‚’ã©ã†é¸ã¶ã‹ã€ã®æ–¹æ³•ãŒé•ã„ã¾ã™ã€‚

#### æ•™è‚²ç”¨å®Ÿè£…ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
```rust
// å˜ç´”ãªæ¸©åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
let probs = softmax(logits / temperature);
let next_token = sample_from_probs(&probs);
```
**ä½•ã‚’ã—ã¦ã„ã‚‹ï¼Ÿ**: ã‚µã‚¤ã‚³ãƒ­ã‚’æŒ¯ã£ã¦æ¬¡ã®å˜èªã‚’é¸ã¶ã ã‘

#### å®Ÿéš›ã®LLMï¼ˆé«˜åº¦ãªæˆ¦ç•¥ï¼‰
```python
# è¤‡æ•°ã®æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã›
def generate(prompt, **kwargs):
    # 1. Top-k samplingï¼ˆä¸Šä½kå€‹ã‹ã‚‰é¸ã¶ï¼‰
    top_k_tokens = get_top_k(logits, k=50)
    # åˆå­¦è€…å‘ã‘: ã€Œä¸€ç•ªå¯èƒ½æ€§ã®é«˜ã”50å€‹ã€ã®ä¸­ã‹ã‚‰é¸ã¶
    
    # 2. Top-p samplingï¼ˆç¢ºç‡ã®é«˜ã„ã‚‚ã®ã‹ã‚‰é¸ã¶ï¼‰
    nucleus_tokens = get_nucleus(logits, p=0.95)
    # åˆå­¦è€…å‘ã‘: ç¢ºç‡ã®åˆè¨ˆ95%ã«ãªã‚‹ã¾ã§ä¸Šä½ã‹ã‚‰é¸ã¶
    
    # 3. Repetition penaltyï¼ˆåŒã˜å˜èªã‚’ç¹°ã‚Šè¿”ã•ãªã„ï¼‰
    logits = apply_repetition_penalty(logits, past_tokens)
    # åˆå­¦è€…å‘ã‘: ã€Œãã®ãã®ãã®...ã€ã®ã‚ˆã†ãªç¹°ã‚Šè¿”ã—ã‚’é˜²ã
    
    # 4. Beam searchï¼ˆè¤‡æ•°ã®å¯èƒ½æ€§ã‚’åŒæ™‚ã«è¿½ã†ï¼‰
    beams = beam_search(logits, beam_size=4)
    # åˆå­¦è€…å‘ã‘: 4ã¤ã®ã€Œç¶šãã®å€™è£œã€ã‚’åŒæ™‚ã«è€ƒãˆã‚‹
    
    # 5. Temperature scalingï¼ˆå‰µé€ æ€§ã®èª¿æ•´ï¼‰
    logits = logits / temperature
    # åˆå­¦è€…å‘ã‘: AIã®ã€Œå†’é™ºå¿ƒã€ã‚’èª¿æ•´
```

**ä¾‹ãˆã‚‹ãªã‚‰**: 
- å­¦ç¿’ç”¨ï¼šãã˜å¼•ãã§å˜èªã‚’é¸ã¶
- å®Ÿéš›ã®LLMï¼šè¤‡æ•°ã®ãƒ«ãƒ¼ãƒ«ã§ã€Œã‚ˆã‚Šè‰¯ã„å˜èªã€ã‚’æ…é‡ã«é¸ã¶

### 3. åœæ­¢æ¡ä»¶ã®é•ã„

#### æ•™è‚²ç”¨å®Ÿè£…
```rust
// å›ºå®šé•·ã¾ãŸã¯EOSãƒˆãƒ¼ã‚¯ãƒ³
if next_token == eos_token_id || length >= max_length {
    break;
}
```

#### å®Ÿéš›ã®LLM
- **è¤‡æ•°ã®åœæ­¢æ¡ä»¶:**
  - EOSãƒˆãƒ¼ã‚¯ãƒ³
  - ç‰¹å®šã®åœæ­¢æ–‡å­—åˆ—ï¼ˆ"\n\n"ã€"```"ãªã©ï¼‰
  - æœ€å¤§é•·
  - ç¹°ã‚Šè¿”ã—æ¤œå‡º
  - ã‚«ã‚¹ã‚¿ãƒ åœæ­¢æ¡ä»¶

- **æ–‡è„ˆä¾å­˜ã®åœæ­¢:**
  ```python
  # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®å ´åˆ
  stop_sequences = ["```", "\n\n", "# End"]
  
  # ä¼šè©±ã®å ´åˆ
  stop_sequences = ["Human:", "User:", "\n\n"]
  ```

### 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†

#### æ•™è‚²ç”¨å®Ÿè£…
```rust
// å˜ç´”ãªåˆ‡ã‚Šè©°ã‚
let context = &tokens[tokens.len().saturating_sub(max_seq_len)..];
```

#### å®Ÿéš›ã®LLM
- **ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: é‡è¦ãªæƒ…å ±ã‚’ä¿æŒ
  - **åˆå­¦è€…å‘ã‘**: å¤ã„æƒ…å ±ã‚’æ¨ã¦ã‚‹æ™‚ã€å¤§äº‹ãªéƒ¨åˆ†ã‚’æ®‹ã™æŠ€è¡“
- **æ³¨æ„æ©Ÿæ§‹ã®æœ€é©åŒ–**: 
  - Flash Attentionï¼ˆè¨ˆç®—ã‚’é«˜é€ŸåŒ–ï¼‰
  - Sparse Attentionï¼ˆå¿…è¦ãªéƒ¨åˆ†ã ã‘è¨ˆç®—ï¼‰
  - RoPE (Rotary Position Embedding)ï¼ˆä½ç½®æƒ…å ±ã®æ–°ã—ã„è¡¨ç¾æ–¹æ³•ï¼‰
- **é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: 
  - GPT-4: 32kï½128kãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç´„100ãƒšãƒ¼ã‚¸åˆ†ã®æ–‡ç« ï¼‰
  - Claude: 100kï½200kãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæœ¬1å†Šåˆ†ï¼‰
  - ç‰¹æ®Šãªä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆALiBiã€RoPEï¼‰

### 5. å“è³ªåˆ¶å¾¡

#### å®Ÿéš›ã®LLMã®è¿½åŠ æ©Ÿèƒ½

**1. ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**
```python
# ä¸é©åˆ‡ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
def filter_tokens(logits, banned_tokens):
    logits[banned_tokens] = -float('inf')
    return logits
```

**2. æ–‡æ³•åˆ¶ç´„**
```python
# JSONãƒ¢ãƒ¼ãƒ‰ãªã©ã€ç‰¹å®šã®å½¢å¼ã‚’å¼·åˆ¶
def constrained_generation(logits, grammar):
    valid_tokens = grammar.get_valid_next_tokens()
    mask = create_mask(valid_tokens)
    return apply_mask(logits, mask)
```

**3. ã‚¬ã‚¤ãƒ‰ä»˜ãç”Ÿæˆ**
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
system_prompt = "You are a helpful assistant..."
few_shot_examples = [...]
```

### 6. æœ€é©åŒ–æŠ€è¡“

#### å®Ÿéš›ã®LLMã®æœ€é©åŒ–

**1. æ¨è«–ã®é«˜é€ŸåŒ–**
- **é‡å­åŒ–**: INT8ã€INT4ã¸ã®å¤‰æ›
  - **åˆå­¦è€…å‘ã‘**: å°æ•°ç‚¹ã®æ•°å€¤ã‚’æ•´æ•°ã«å¤‰æ›ã—ã¦è¨ˆç®—ã‚’é€Ÿãã™ã‚‹
- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥**: è¨ˆç®—æ¸ˆã¿ã®Key-Valueã‚’å†åˆ©ç”¨
  - **åˆå­¦è€…å‘ã‘**: ä¸€åº¦è¨ˆç®—ã—ãŸçµæœã‚’ä¿å­˜ã—ã¦ã€åŒã˜è¨ˆç®—ã‚’ç¹°ã‚Šè¿”ã•ãªã„
- **ãƒãƒƒãƒå‡¦ç†**: è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åŒæ™‚å‡¦ç†
  - **åˆå­¦è€…å‘ã‘**: 10äººã‹ã‚‰ã®è³ªå•ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼ˆä¸€äººãšã¤ã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰

**2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**
```python
# KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¾‹
class KVCache:
    def __init__(self):
        self.keys = []
        self.values = []
    
    def update(self, new_keys, new_values):
        self.keys.append(new_keys)
        self.values.append(new_values)
```

**3. æŠ•æ©Ÿçš„ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**
- å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§å€™è£œã‚’ç”Ÿæˆ
- å¤§ããªãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼
- é«˜é€ŸåŒ–ã‚’å®Ÿç¾
  - **åˆå­¦è€…å‘ã‘**: ã€Œä¸‹æ›¸ãã€ã‚’å…ˆè¼©ãŒæ›¸ã„ã¦ã€ã€Œæ¸…æ›¸ã€ã‚’å…ˆç”ŸãŒãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚ˆã†ãªã‚‚ã®

### 7. å®Ÿè£…ä¾‹ï¼šå®Ÿéš›ã®LLMã«è¿‘ã„ç”Ÿæˆé–¢æ•°

```python
def advanced_generate(
    prompt: str,
    max_length: int = 100,
    temperature: float = 0.8,
    top_k: int = 50,
    top_p: float = 0.95,
    repetition_penalty: float = 1.1,
    stop_sequences: List[str] = None,
):
    tokens = tokenizer.encode(prompt)
    past_key_values = None
    
    for _ in range(max_length):
        # 1. Forward pass with KV cache
        logits, past_key_values = model(tokens, past_key_values)
        
        # 2. Apply repetition penalty
        for token in set(tokens):
            logits[-1, token] /= repetition_penalty
        
        # 3. Top-k filtering
        top_k_logits, top_k_indices = torch.topk(logits[-1], top_k)
        
        # 4. Top-p (nucleus) filtering
        sorted_logits, sorted_indices = torch.sort(top_k_logits, descending=True)
        cumsum = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
        mask = cumsum <= top_p
        top_p_logits = sorted_logits[mask]
        
        # 5. Temperature sampling
        probs = torch.softmax(top_p_logits / temperature, dim=-1)
        next_token = torch.multinomial(probs, 1)
        
        # 6. Check stop conditions
        decoded = tokenizer.decode(tokens + [next_token])
        if any(stop_seq in decoded for stop_seq in stop_sequences):
            break
            
        tokens.append(next_token)
    
    return tokenizer.decode(tokens)
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| ç‰¹å¾´ | æ•™è‚²ç”¨å®Ÿè£… | å®Ÿéš›ã®LLM |
|------|------------|-----------|
| ãƒˆãƒ¼ã‚¯ãƒ³åŒ– | ç©ºç™½åŒºåˆ‡ã‚Š | BPE/SentencePiece |
| èªå½™ã‚µã‚¤ã‚º | ~100 | 30,000ï½100,000 |
| ç”Ÿæˆé€Ÿåº¦ | é…ã„ | é«˜é€Ÿï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰ |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | åŠ¹ç‡çš„ã§ãªã„ | é«˜åº¦ã«æœ€é©åŒ– |
| ç”Ÿæˆå“è³ª | åŸºæœ¬çš„ | é«˜å“è³ª |
| åˆ¶å¾¡æ©Ÿèƒ½ | é™å®šçš„ | è±Šå¯Œ |

## ğŸ”§ å®Ÿè£…ã®æ”¹å–„æ¡ˆ

æ•™è‚²ç”¨å®Ÿè£…ã‚’å®Ÿéš›ã®LLMã«è¿‘ã¥ã‘ã‚‹ã«ã¯ï¼š

1. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ”¹å–„**
   - BPEã®å®Ÿè£…
   - ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²
   - ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ç®¡ç†

2. **ç”Ÿæˆæˆ¦ç•¥ã®æ‹¡å¼µ**
   - Top-k/Top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
   - Beam search
   - ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£

3. **æœ€é©åŒ–ã®å®Ÿè£…**
   - KVã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - ãƒãƒƒãƒå‡¦ç†
   - é‡å­åŒ–

4. **å“è³ªåˆ¶å¾¡**
   - åœæ­¢æ¡ä»¶ã®æ‹¡å¼µ
   - ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
   - å‡ºåŠ›æ¤œè¨¼

ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’æ®µéšçš„ã«å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå®Ÿç”¨çš„ãªLLMã«è¿‘ã¥ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚