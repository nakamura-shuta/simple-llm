# 処理フロー説明

このドキュメントでは、simple-llm（教育用Transformerモデル実装）の処理の流れを詳しく説明します。

**初学者向け説明**: このドキュメントでは、プログラムがどのような順番で動作するかを説明します。料理のレシピのように、材料（データ）を準備して、調理（処理）して、完成品（生成されたテキスト）を作る流れを見ていきましょう。

## 概要

本プロジェクトは、GPT-2スタイルのTransformerモデルをRustで実装した教育用プロジェクトです。

**わかりやすく言うと**: ChatGPTの簡易版を作って、その仕組みを理解しようというプロジェクトです。実際に文章を学習して、新しい文章を生成できます。

## プログラムの全体的な流れ

### 1. 起動時の処理

```
main.rs → TrainableTransformer初期化
```

**初学者向け**: プログラムを起動すると、まず準備作業が行われます。

1. **データセット読み込み** (`data.txt`)
   - 「データセット」= モデルが学習するための教材
   - 日本語のシンプルなテキストデータを読み込み
   - 各行が1つの学習材料として扱われる
   - 例：「こんにちは 世界」「今日は良い天気ですね」など

2. **モデル初期化**（モデル = 言語を理解・生成する頭脳）
   - `TrainableTransformer::from_texts()`でモデルを作成
   - テキストデータから自動的に「語彙」（知っている単語のリスト）を構築
   - Transformerアーキテクチャの各部品を準備

### 2. トークナイザーの処理

```
テキスト → トークン化 → 数値ID列
```

**初学者向け**: 「トークナイザー」は、人間の言葉をコンピュータが理解できる数字に変換する翻訳機のようなものです。

**SimpleTokenizer**の動作:
1. **語彙構築** (`build_vocab`)
   - 訓練データから単語を抽出
   - 各単語に番号（ID）を割り当て（例：「こんにちは」=4、「世界」=5）
   - 特殊トークン（特別な記号）:
     - `<bos>` = 文の始まり（Begin Of Sentence）
     - `<eos>` = 文の終わり（End Of Sentence）
     - `<unk>` = 知らない単語（Unknown）

2. **エンコード処理**（文字→数字への変換）
   ```
   "こんにちは 世界" → [<bos>, こんにちは, 世界, <eos>] → [1, 4, 5, 0]
   ```

3. **デコード処理**（数字→文字への変換）
   ```
   [1, 4, 5, 0] → ["こんにちは", "世界"] → "こんにちは 世界"
   ```

### 3. Transformerモデルの順伝播

```
入力トークン → 埋め込み → Transformerレイヤー → 出力確率
```

#### 3.1 埋め込み層
```rust
hidden_states = token_embedding + position_embedding
```
- **トークン埋め込み**: 各単語の意味を表現するベクトル
  - **ベクトルとは**: 数字のリスト。例：「猫」=[0.2, -0.5, 0.8...]のように単語を数値化
  - **次元とは**: ベクトルの長さ。「64次元」=64個の数字で単語を表現
- **位置埋め込み**: シーケンス内の位置情報を表現
  - 「私は学生です」と「学生は私です」の違いを理解するために必要

#### 3.2 Transformerレイヤー（各レイヤーで繰り返し）

1. **Layer Normalization (Pre-LN)**
   ```rust
   x_norm = LayerNorm(x)
   ```

2. **Multi-Head Attention**
   ```rust
   Q = x_norm @ W_q  // Query（質問）
   K = x_norm @ W_k  // Key（鍵）  
   V = x_norm @ W_v  // Value（値）
   
   scores = (Q @ K.T) / sqrt(head_dim)
   scores = apply_causal_mask(scores)  // 未来の情報をマスク
   attn_weights = softmax(scores)
   attn_output = attn_weights @ V
   output = attn_output @ W_o
   ```
   
   **初学者向け説明**:
   - **@**: 行列の掛け算（数学のやや難しい計算）
   - **Q,K,V**: 「どの単語に注目するか」を決める3つの役割
   - **causal_mask**: 「未来の単語を見ない」ルール。カンニング防止
   - **softmax**: 確率に変換（合計が1になるように調整）

3. **残差接続**
   ```rust
   x = x + attn_output
   ```
   **初学者向け**: 「元の情報」+「新しい情報」を保持。忘れにくい仕組み

4. **Feed-Forward Network**
   ```rust
   x_norm2 = LayerNorm(x)
   ffn_output = GELU(x_norm2 @ W1 + b1) @ W2 + b2
   x = x + ffn_output
   ```
   **初学者向け**:
   - **FFN**: 各位置の情報を個別に処理する「脳の回路」
   - **GELU**: 活性化関数（情報を「オン/オフ」するスイッチ）
   - **W1, b1, W2, b2**: 学習で調整されるパラメータ（重みとバイアス）

#### 3.3 最終出力
```rust
x = LayerNorm(x)
logits = x @ output_projection  // 語彙サイズへの投影
```
**初学者向け**: 
- **logits**: 各単語の「出やすさ」の生のスコア
- **語彙サイズへの投影**: 「知っている全単語」の中から選ぶ準備

### 4. 学習処理

```
順伝播 → 損失計算 → 重み更新
```

1. **損失計算**
   - クロスエントロピー損失を使用
   - 予測した次の単語の確率と正解を比較
   - **初学者向け**: 「どれだけ間違えたか」を数値化

2. **重み更新**（逆伝播実装）
   - 埋め込み層と出力層の勾配を計算
   - 勾配降下法で重みを更新
   - **逆伝播とは**: 間違いから「どこをどう直すか」を計算する方法
   - **勾配とは**: 「どの方向にどれだけ直すか」の指示書

### 5. テキスト生成

```
プロンプト → トークン化 → 逐次生成 → デコード
```

1. **初期化**
   - プロンプトをトークン化
   - コンテキストウィンドウに収める

2. **逐次生成**
   ```rust
   while not done:
       logits = model.forward(context)
       probs = softmax(logits[-1] / temperature)
       next_token = sample_from_probs(probs)
       context.append(next_token)
   ```

3. **温度サンプリング**
   - `temperature`パラメータで生成の多様性を制御
   - 低い温度（0.1）: より決定的な出力（「安全運転」）
   - 高い温度（1.0）: より多様な出力（「冒険運転」）
   - **初学者向け**: AIの「創造性」を調整するつまみ

## ファイル構成と役割

### コアモジュール

1. **`working_transformer.rs`**
   - 実際に動作するTransformer実装
   - Multi-Head Attention、Layer Norm、FFNを含む完全な実装
   - トークナイザーも含む統合実装

### 実行フロー

```
main.rs
├── データ読み込み
├── TrainableTransformer初期化
│   ├── SimpleTokenizer作成
│   └── WorkingTransformer作成
├── 学習ループ
│   └── model.train()
└── インタラクティブ生成
    └── model.generate()
```

## 重要な設計上の選択

1. **教育目的の簡潔性**
   - 理解しやすさを優先した設計
   - 本格的な最適化よりも明確さを重視

2. **Pre-LN構成**
   - Layer Normalizationを各サブレイヤーの前に配置
   - 安定した学習のため
   - **Layer Normalizationとは**: 数値を「標準化」して計算を安定させる技術

3. **Causal Masking**
   - 自己回帰的な生成のため、未来の情報をマスク
   - 各位置は自分より前の位置のみ参照可能
   - **初学者向け**: 「答えを先に見ない」ルール。次の単語を予測する時にカンニング防止

4. **基本的な逆伝播による学習**
   - 埋め込み層と出力層の勾配計算を実装
   - クロスエントロピー損失に基づく勾配降下法
   - 実際に損失が減少し、パターンを学習可能

## デバッグとトラブルシューティング

### よくある問題

1. **次元の不一致**
   - 埋め込みサイズとヘッド数の関係を確認
   - `hidden_size % num_heads == 0`である必要

2. **メモリ使用量**
   - 大きなモデルサイズは避ける
   - 教育用として小さな設定を使用
   - **メモリとは**: コンピュータの「作業机」の広さ

3. **生成品質**
   - 十分な学習エポック数が必要
   - データセットの品質と量が重要

### パフォーマンス考慮事項

- 実装は教育目的のため、最適化よりも理解しやすさを優先
- 実用的な用途には、より最適化された実装を使用すべき

## まとめ

このプロジェクトは、Transformerアーキテクチャの基本的な動作原理を理解するための教育用実装です。実際のGPT-2と比較して大幅に簡略化されていますが、核となる概念（自己注意機構、位置エンコーディング、自己回帰生成）は忠実に実装されています。